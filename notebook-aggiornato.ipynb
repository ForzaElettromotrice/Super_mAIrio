{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8488567,"sourceType":"datasetVersion","datasetId":5064048}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-23T05:04:32.808488Z","iopub.execute_input":"2024-05-23T05:04:32.809104Z","iopub.status.idle":"2024-05-23T05:04:32.825008Z","shell.execute_reply.started":"2024-05-23T05:04:32.809069Z","shell.execute_reply":"2024-05-23T05:04:32.824116Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/lib/kaggle/gcp.py\n/kaggle/input/checkpoint2/mario_net_4.chkpt\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install gym-super-mario-bros","metadata":{"execution":{"iopub.status.busy":"2024-05-23T08:29:21.359529Z","iopub.execute_input":"2024-05-23T08:29:21.359870Z","iopub.status.idle":"2024-05-23T08:29:43.685911Z","shell.execute_reply.started":"2024-05-23T08:29:21.359841Z","shell.execute_reply":"2024-05-23T08:29:43.684778Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting gym-super-mario-bros\n  Downloading gym_super_mario_bros-7.4.0-py3-none-any.whl.metadata (10 kB)\nCollecting nes-py>=8.1.4 (from gym-super-mario-bros)\n  Downloading nes_py-8.2.1.tar.gz (77 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.7/77.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: gym>=0.17.2 in /opt/conda/lib/python3.10/site-packages (from nes-py>=8.1.4->gym-super-mario-bros) (0.26.2)\nRequirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.10/site-packages (from nes-py>=8.1.4->gym-super-mario-bros) (1.26.4)\nCollecting pyglet<=1.5.21,>=1.4.0 (from nes-py>=8.1.4->gym-super-mario-bros)\n  Downloading pyglet-1.5.21-py3-none-any.whl.metadata (7.6 kB)\nRequirement already satisfied: tqdm>=4.48.2 in /opt/conda/lib/python3.10/site-packages (from nes-py>=8.1.4->gym-super-mario-bros) (4.66.1)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros) (2.2.1)\nRequirement already satisfied: gym-notices>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros) (0.0.8)\nDownloading gym_super_mario_bros-7.4.0-py3-none-any.whl (199 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyglet-1.5.21-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: nes-py\n  Building wheel for nes-py (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for nes-py: filename=nes_py-8.2.1-cp310-cp310-linux_x86_64.whl size=48172 sha256=b438f1e935e1d4dfba4cadfd15635e3d096e8848ef90912b34fe501bcdc1cfca\n  Stored in directory: /root/.cache/pip/wheels/34/a7/d5/9aa14b15df740a53d41f702e4c795731b6c4da7925deb8476c\nSuccessfully built nes-py\nInstalling collected packages: pyglet, nes-py, gym-super-mario-bros\nSuccessfully installed gym-super-mario-bros-7.4.0 nes-py-8.2.1 pyglet-1.5.21\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install git+https://github.com/pytorch-labs/tensordict\n!pip install git+https://github.com/pytorch/rl.git","metadata":{"execution":{"iopub.status.busy":"2024-05-23T08:23:38.814958Z","iopub.execute_input":"2024-05-23T08:23:38.815304Z","iopub.status.idle":"2024-05-23T08:28:57.182467Z","shell.execute_reply.started":"2024-05-23T08:23:38.815276Z","shell.execute_reply":"2024-05-23T08:28:57.181371Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/pytorch-labs/tensordict\n  Cloning https://github.com/pytorch-labs/tensordict to /tmp/pip-req-build-ub3841fm\n  Running command git clone --filter=blob:none --quiet https://github.com/pytorch-labs/tensordict /tmp/pip-req-build-ub3841fm\n  Resolved https://github.com/pytorch-labs/tensordict to commit 4a34b51f247d7c358602076b676667429950cdcd\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from tensordict==0.4.0+4a34b51) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from tensordict==0.4.0+4a34b51) (1.26.4)\nRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from tensordict==0.4.0+4a34b51) (2.2.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->tensordict==0.4.0+4a34b51) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->tensordict==0.4.0+4a34b51) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->tensordict==0.4.0+4a34b51) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->tensordict==0.4.0+4a34b51) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->tensordict==0.4.0+4a34b51) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->tensordict==0.4.0+4a34b51) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->tensordict==0.4.0+4a34b51) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->tensordict==0.4.0+4a34b51) (1.3.0)\nBuilding wheels for collected packages: tensordict\n  Building wheel for tensordict (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for tensordict: filename=tensordict-0.4.0+4a34b51-cp310-cp310-linux_x86_64.whl size=299108 sha256=9b762e8fdbde6dc023e00d7c7831b5f3e527425007bb4aa8ac75abaf9fe6a26b\n  Stored in directory: /tmp/pip-ephem-wheel-cache-ryhk4j92/wheels/3e/51/3b/af742bd6c05b4e3a7fde597889cfcb9399bfd29535fff78f7b\nSuccessfully built tensordict\nInstalling collected packages: tensordict\nSuccessfully installed tensordict-0.4.0+4a34b51\nCollecting git+https://github.com/pytorch/rl.git\n  Cloning https://github.com/pytorch/rl.git to /tmp/pip-req-build-xhggv72z\n  Running command git clone --filter=blob:none --quiet https://github.com/pytorch/rl.git /tmp/pip-req-build-xhggv72z\n  Resolved https://github.com/pytorch/rl.git to commit a93063b2fd166c5c9be9faafd55ff0dc642d3a66\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from torchrl==0.4.0+a93063b) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchrl==0.4.0+a93063b) (1.26.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from torchrl==0.4.0+a93063b) (21.3)\nRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from torchrl==0.4.0+a93063b) (2.2.1)\nRequirement already satisfied: tensordict>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from torchrl==0.4.0+a93063b) (0.4.0+4a34b51)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->torchrl==0.4.0+a93063b) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->torchrl==0.4.0+a93063b) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->torchrl==0.4.0+a93063b) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->torchrl==0.4.0+a93063b) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->torchrl==0.4.0+a93063b) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->torchrl==0.4.0+a93063b) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->torchrl==0.4.0+a93063b) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->torchrl==0.4.0+a93063b) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->torchrl==0.4.0+a93063b) (1.3.0)\nBuilding wheels for collected packages: torchrl\n  Building wheel for torchrl (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for torchrl: filename=torchrl-0.4.0+a93063b-cp310-cp310-linux_x86_64.whl size=934917 sha256=0e2e23952d7b8905ac873d01e11f2dc3b05435da59b2d560c33decc14d55d770\n  Stored in directory: /tmp/pip-ephem-wheel-cache-h1tl6b7e/wheels/bf/50/1b/b2bc7c8b862b1f514bc93c6ef3f68577ee6cf5d2d87796738f\nSuccessfully built torchrl\nInstalling collected packages: torchrl\nSuccessfully installed torchrl-0.4.0+a93063b\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip uninstall pyglet -y\n!pip install pyglet==1.5.21","metadata":{"execution":{"iopub.status.busy":"2024-05-22T07:43:05.424764Z","iopub.execute_input":"2024-05-22T07:43:05.425139Z","iopub.status.idle":"2024-05-22T07:43:14.903260Z","shell.execute_reply.started":"2024-05-22T07:43:05.425101Z","shell.execute_reply":"2024-05-22T07:43:14.902052Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"^C\nTraceback (most recent call last):\n  File \"/opt/conda/bin/pip\", line 6, in <module>\n    from pip._internal.cli.main import main\n  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/cli/main.py\", line 10, in <module>\n    from pip._internal.cli.autocompletion import autocomplete\n  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\n    from pip._internal.cli.main_parser import create_main_parser\n  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/cli/main_parser.py\", line 9, in <module>\n    from pip._internal.build_env import get_runnable_pip\n  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/build_env.py\", line 19, in <module>\n    from pip._internal.cli.spinners import open_spinner\n  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/cli/spinners.py\", line 9, in <module>\n    from pip._internal.utils.logging import get_indentation\n  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/utils/logging.py\", line 22, in <module>\n    from pip._vendor.rich.logging import RichHandler\n  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/rich/logging.py\", line 15, in <module>\n    from .traceback import Traceback\n  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/rich/traceback.py\", line 185, in <module>\n    class _SyntaxError:\n  File \"/opt/conda/lib/python3.10/dataclasses.py\", line 1184, in dataclass\n    return wrap(cls)\n  File \"/opt/conda/lib/python3.10/dataclasses.py\", line 1175, in wrap\n    return _process_class(cls, init, repr, eq, order, unsafe_hash,\n  File \"/opt/conda/lib/python3.10/dataclasses.py\", line 1053, in _process_class\n    _cmp_fn('__eq__', '==',\n  File \"/opt/conda/lib/python3.10/dataclasses.py\", line 629, in _cmp_fn\n    return _create_fn(name,\n  File \"/opt/conda/lib/python3.10/dataclasses.py\", line 432, in _create_fn\n    exec(txt, globals, ns)\n  File \"<string>\", line 1, in <module>\nKeyboardInterrupt\nRequirement already satisfied: pyglet==1.5.21 in /opt/conda/lib/python3.10/site-packages (1.5.21)\n^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import datetime\nfrom pathlib import Path\n\nimport gym\nimport gym_super_mario_bros\nimport torch\nfrom gym.wrappers import FrameStack\nfrom nes_py.wrappers import JoypadSpace\n\nfrom torch import nn\nimport numpy as np\nfrom gym.spaces import Box\nfrom torchvision import transforms as T\n\nfrom tensordict import TensorDict\nfrom torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage","metadata":{"execution":{"iopub.status.busy":"2024-05-23T08:33:44.841748Z","iopub.execute_input":"2024-05-23T08:33:44.842159Z","iopub.status.idle":"2024-05-23T08:33:51.514624Z","shell.execute_reply.started":"2024-05-23T08:33:44.842126Z","shell.execute_reply":"2024-05-23T08:33:51.513684Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchrl/data/replay_buffers/samplers.py:37: UserWarning: Failed to import torchrl C++ binaries. Some modules (eg, prioritized replay buffers) may not work with your installation. If you installed TorchRL from PyPI, please report the bug on TorchRL github. If you installed TorchRL locally and/or in development mode, check that you have all the required compiling packages.\n  warnings.warn(EXTENSION_WARNING)\n","output_type":"stream"}]},{"cell_type":"code","source":"class MarioNet(nn.Module):\n    \"\"\"mini CNN structure\n  input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n  \"\"\"\n\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        c, h, w = input_dim\n\n        if h != 84:\n            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n        if w != 84:\n            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n\n        self.online = self.__build_cnn(c, output_dim)\n\n        self.target = self.__build_cnn(c, output_dim)\n        self.target.load_state_dict(self.online.state_dict())\n\n        # Q_target parameters are frozen.\n        for p in self.target.parameters():\n            p.requires_grad = False\n\n    def forward(self, input, model):\n        if model == \"online\":\n            return self.online(input)\n        elif model == \"target\":\n            return self.target(input)\n\n    def __build_cnn(self, c, output_dim):\n        return nn.Sequential(\n            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(3136, 512),\n            nn.ReLU(),\n            nn.Linear(512, output_dim),\n        )\n        \n    def td_estimate(self, state, action):\n        current_Q = self.net(state, model=\"online\")[\n            np.arange(0, self.batch_size), action\n        ]  # Q_online(s,a)\n        return current_Q\n\n    @torch.no_grad()\n    def td_target(self, reward, next_state, done):\n        next_state_Q = self.net(next_state, model=\"online\")\n        best_action = torch.argmax(next_state_Q, axis=1)\n        next_Q = self.net(next_state, model=\"target\")[\n            np.arange(0, self.batch_size), best_action\n        ]\n        return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n\nclass Mario:\n    def __init__(self, state_dim, action_dim, save_dir, checkpoint = None):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.save_dir = save_dir\n\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n        # Mario's DNN to predict the most optimal action - we implement this in the Learn section\n        self.net = MarioNet(self.state_dim, self.action_dim).float()\n        self.net = self.net.to(device = self.device)\n\n        self.exploration_rate = 1\n        self.exploration_rate_decay = 0.99999975\n        self.exploration_rate_min = 0.1\n        self.curr_step = 0\n\n        self.save_every = 1e4  # no. of experiences between saving Mario Net\n\n        self.memory = TensorDictReplayBuffer(storage = LazyMemmapStorage(50000, device = torch.device(\"cpu\")))\n        self.batch_size = 32\n\n        self.gamma = 0.9\n\n        self.use_cuda = torch.cuda.is_available()\n        if checkpoint:\n            self.load(checkpoint)\n\n        self.optimizer = torch.optim.Adam(self.net.parameters(), lr = 0.00025)\n        # self.loss_fn = torch.nn.SmoothL1Loss()\n        self.loss_fn = torch.nn.MSELoss()\n\n        self.burnin = 1e4  # min. experiences before training\n        self.learn_every = 3  # no. of experiences between updates to Q_online\n        self.sync_every = 1e4  # no. of experiences between Q_target & Q_online sync\n\n    def act(self, state):\n        \"\"\"\n    Given a state, choose an epsilon-greedy action and update value of step.\n\n    Inputs:\n    state(``LazyFrame``): A single observation of the current state, dimension is (state_dim)\n    Outputs:\n    ``action_idx`` (``int``): An integer representing which action Mario will perform\n    \"\"\"\n        # EXPLORE\n        if np.random.rand() < self.exploration_rate:\n            action_idx = np.random.randint(self.action_dim)\n\n        # EXPLOIT\n        else:\n            state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n            state = torch.tensor(state, device = self.device).unsqueeze(0)\n            action_values = self.net(state, model = \"online\")\n            action_idx = torch.argmax(action_values, axis = 1).item()\n\n        # decrease exploration_rate\n        self.exploration_rate *= self.exploration_rate_decay\n        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n\n        # increment step\n        self.curr_step += 1\n        return action_idx\n\n    def cache(self, state, next_state, action, reward, done):\n        \"\"\"\n        Store the experience to self.memory (replay buffer)\n\n        Inputs:\n        state (``LazyFrame``),\n        next_state (``LazyFrame``),\n        action (``int``),\n        reward (``float``),\n        done(``bool``))\n        \"\"\"\n\n        def first_if_tuple(x):\n            return x[0] if isinstance(x, tuple) else x\n\n        state = first_if_tuple(state).__array__()\n        next_state = first_if_tuple(next_state).__array__()\n\n        state = torch.tensor(state)\n        next_state = torch.tensor(next_state)\n        action = torch.tensor([action])\n        reward = torch.tensor([reward])\n        done = torch.tensor([done])\n\n        # self.memory.append((state, next_state, action, reward, done,))\n        self.memory.add(TensorDict({ \"state\": state, \"next_state\": next_state, \"action\": action, \"reward\": reward, \"done\": done }, batch_size = []))\n\n    def recall(self):\n        \"\"\"\n        Retrieve a batch of experiences from memory\n        \"\"\"\n        batch = self.memory.sample(self.batch_size).to(self.device)\n        state, next_state, action, reward, done = (batch.get(key) for key in (\"state\", \"next_state\", \"action\", \"reward\", \"done\"))\n        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n\n    def update_Q_online(self, td_estimate, td_target):\n        loss = self.loss_fn(td_estimate, td_target)\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        return loss.item()\n\n    def sync_Q_target(self):\n        self.net.target.load_state_dict(self.net.online.state_dict())\n\n    def save(self):\n        save_path = (\n                self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n        )\n        torch.save(\n            dict(model = self.net.state_dict(), exploration_rate = self.exploration_rate),\n            save_path,\n        )\n        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")\n\n    def learn(self):\n        if self.curr_step % self.sync_every == 0:\n            self.sync_Q_target()\n\n        if self.curr_step % self.save_every == 0:\n            self.save()\n\n        if self.curr_step < self.burnin:\n            return None, None\n\n        if self.curr_step % self.learn_every != 0:\n            return None, None\n\n        # Sample from memory\n        state, next_state, action, reward, done = self.recall()\n\n        # Get TD Estimate\n        td_est = self.td_estimate(state, action)\n\n        # Get TD Target\n        td_tgt = self.td_target(reward, next_state, done)\n\n        # Backpropagate loss through Q_online\n        loss = self.update_Q_online(td_est, td_tgt)\n\n        return (td_est.mean().item(), loss)\n\n    def td_estimate(self, state, action):\n        current_Q = self.net(state, model = \"online\")[\n            np.arange(0, self.batch_size), action\n        ]  # Q_online(s,a)\n        return current_Q\n\n    @torch.no_grad()\n    def td_target(self, reward, next_state, done):\n        next_state_Q = self.net(next_state, model = \"online\")\n        best_action = torch.argmax(next_state_Q, axis = 1)\n        next_Q = self.net(next_state, model = \"target\")[\n            np.arange(0, self.batch_size), best_action\n        ]\n        return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n\n    def load(self, load_path):\n        if not load_path.exists():\n            raise ValueError(f\"{load_path} does not exist\")\n\n        ckp = torch.load(load_path, map_location = ('cuda' if self.use_cuda else 'cpu'))\n        exploration_rate = ckp.get('exploration_rate')\n        state_dict = ckp.get('model')\n\n        print(f\"Loading model at {load_path} with exploration rate {exploration_rate}\")\n        self.net.load_state_dict(state_dict)\n        self.exploration_rate = exploration_rate\n\n\nclass SkipFrame(gym.Wrapper):\n    def __init__(self, env, skip):\n        \"\"\"Return only every `skip`-th frame\"\"\"\n        super().__init__(env)\n        self._skip = skip\n\n    def step(self, action):\n        \"\"\"Repeat action, and sum reward\"\"\"\n        total_reward = 0.0\n        for i in range(self._skip):\n            # Accumulate reward and repeat the same action\n            obs, reward, done, trunk, info = self.env.step(action)\n            total_reward += reward\n            if done:\n                break\n        return obs, total_reward, done, trunk, info\n\n\nclass GrayScaleObservation(gym.ObservationWrapper):\n    def __init__(self, env):\n        super().__init__(env)\n        obs_shape = self.observation_space.shape[:2]\n        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n\n    def permute_orientation(self, observation):\n        # permute [H, W, C] array to [C, H, W] tensor\n        observation = np.transpose(observation, (2, 0, 1))\n        observation = torch.tensor(observation.copy(), dtype=torch.float)\n        return observation\n\n    def observation(self, observation):\n        observation = self.permute_orientation(observation)\n        transform = T.Grayscale()\n        observation = transform(observation)\n        return observation\n\n\nclass ResizeObservation(gym.ObservationWrapper):\n    def __init__(self, env, shape):\n        super().__init__(env)\n        if isinstance(shape, int):\n            self.shape = (shape, shape)\n        else:\n            self.shape = tuple(shape)\n\n        obs_shape = self.shape + self.observation_space.shape[2:]\n        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n\n    def observation(self, observation):\n        transforms = T.Compose(\n            [T.Resize(self.shape, antialias=True), T.Normalize(0, 255)]\n        )\n        observation = transforms(observation).squeeze(0)\n        return observation\n\n\ndef custom_reward(old, new):\n    if old is None:\n        return 0\n\n    v = new[\"x_pos\"] - old[\"x_pos\"]\n    c = new[\"time\"] - old[\"time\"]\n    d = old[\"life\"] - new[\"life\"]\n    # p = new[\"score\"] - old[\"score\"]\n    reward = 0\n\n    # reward += p / 2\n\n    # if new[\"status\"] == \"tall\" and old[\"status\"] == \"small\":\n    #    reward += 500\n\n    # if v == 0:\n    #    reward -= 10\n\n    reward += v * 3 + c\n    \n    if d == 1:\n        reward = -30\n\n    return reward\n\n\n\n\ndef main():\n    # print(gym.__version__) # 0.26.2\n    # JoypadSpace.reset = lambda self, **kwargs: self.env.reset(**kwargs)\n    env = gym_super_mario_bros.make('SuperMarioBros-v0', apply_api_compatibility = True)\n\n    # Apply Wrappers to environment\n    env = SkipFrame(env, skip = 4)\n    env = GrayScaleObservation(env)\n    env = ResizeObservation(env, shape = 84)\n    if gym.__version__ < '0.26':\n        env = FrameStack(env, num_stack = 4, new_step_api = True)\n    else:\n        env = FrameStack(env, num_stack = 4)\n\n    # env = JoypadSpace(env, SIMPLE_MOVEMENT)\n    # Limit the action-space to\n    #   0. walk right\n    #   1. jump right\n    # env = JoypadSpace(env, [['right'], ['right', 'A'], ['left'], ['A'], ['left', 'A']])\n    env = JoypadSpace(env, [['right'], ['right', 'A']])\n    # done = True\n    # clock = pygame.time.Clock()\n    # for step in range(5000):\n    #     clock.tick(24)\n    #     if done:\n    #         state = env.reset()\n    #     state, reward, terminated, truncated, info = env.step(env.action_space.sample())\n    #     done = terminated or truncated\n\n    #     # state = cv2.cvtColor(state, cv2.COLOR_RGB2GRAY)\n    #     # state = cv2.resize(state, (84, 84))\n\n    #     cv2.imshow(\"State\", state)\n\n    #     env.render()\n\n    # env.close() \n    use_cuda = torch.cuda.is_available()\n    print(f\"Using CUDA: {use_cuda}\\n\")\n\n    save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n    save_dir.mkdir(parents = True)\n\n    checkpoint = Path(\"/kaggle/input/checkpoint2/mario_net_4.chkpt\")\n    # checkpoint = None\n\n    mario = Mario(state_dim = (4, 84, 84), action_dim = env.action_space.n, save_dir = save_dir, checkpoint = checkpoint)\n\n    # logger = MetricLogger(save_dir)\n    mario.burnin = 32\n    episodes = 5000\n    # mario.exploration_rate = 0.4\n    for e in range(episodes):\n        state = env.reset()\n\n        # Play the game!\n        old_info = None\n        total_reward = 0\n        max_x = 0\n\n        while True:\n\n            # Run agent on the state\n            action = mario.act(state)\n\n            # Agent performs action\n            next_state, reward, terminated, truncated, info = env.step(action)\n            # print(reward)\n            # reward = custom_reward(old_info, info)\n            # print(reward)\n            # old_info = info\n            # print(mario.curr_step)\n            # print(reward, info)\n            done = terminated or truncated\n            # next_state, reward, done, trunc, info = env.step(action)\n            if mario.curr_step % 5000 == 0:\n                print(mario.exploration_rate)\n\n            # Remember\n            mario.cache(state, next_state, action, reward, done)\n\n            # Learn\n            q, loss = mario.learn()\n\n            # Logging\n            # logger.log_step(reward, loss, q)\n            # print(reward, loss, q)\n\n            # Update state\n            state = next_state\n            \n            total_reward += reward\n            max_x = max(max_x, info[\"x_pos\"])\n\n            # Check if end of game\n            if done or info[\"flag_get\"]:\n                if info[\"flag_get\"]:\n                    print(\"HA FINITO\")\n                break\n        \n        print(f\"Episode {e + 1} Total reward: {total_reward}, Max x: {max_x}\")\n        # logger.log_episode()\n\n        # if (e % 20 == 0) or (e == episodes - 1):\n        #     logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T08:34:46.437385Z","iopub.execute_input":"2024-05-23T08:34:46.437798Z","iopub.status.idle":"2024-05-23T08:34:46.639137Z","shell.execute_reply.started":"2024-05-23T08:34:46.437769Z","shell.execute_reply":"2024-05-23T08:34:46.638165Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\nif __name__ == '__main__':\n    main()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T08:34:53.242410Z","iopub.execute_input":"2024-05-23T08:34:53.243025Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Using CUDA: True\n\nLoading model at /kaggle/input/checkpoint2/mario_net_4.chkpt with exploration rate 0.3901239635908362\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n  if not isinstance(terminated, (bool, np.bool8)):\n","output_type":"stream"},{"name":"stdout","text":"Episode 1 Total reward: 2280.0, Max x: 1138\nEpisode 2 Total reward: 1252.0, Max x: 899\nEpisode 3 Total reward: 1473.0, Max x: 1135\nEpisode 4 Total reward: 2253.0, Max x: 1419\nEpisode 5 Total reward: 2652.0, Max x: 1138\nEpisode 6 Total reward: 2245.0, Max x: 1135\nEpisode 7 Total reward: 1530.0, Max x: 1497\nEpisode 8 Total reward: 2931.0, Max x: 1406\nEpisode 9 Total reward: 1740.0, Max x: 898\nEpisode 10 Total reward: 1859.0, Max x: 1133\n0.38963661323279\nEpisode 11 Total reward: 3914.0, Max x: 2758\nEpisode 12 Total reward: 2690.0, Max x: 1139\nEpisode 13 Total reward: 1802.0, Max x: 898\nEpisode 14 Total reward: 2922.0, Max x: 1410\nEpisode 15 Total reward: 1421.0, Max x: 1415\nEpisode 16 Total reward: 1431.0, Max x: 1413\nEpisode 17 Total reward: 2857.0, Max x: 1137\nEpisode 18 Total reward: 2002.0, Max x: 877\nEpisode 19 Total reward: 2261.0, Max x: 1125\nEpisode 20 Total reward: 671.0, Max x: 296\nEpisode 21 Total reward: 1684.0, Max x: 1502\n0.3891498716821836\nMarioNet saved to checkpoints/2024-05-23T08-34-53/mario_net_1.chkpt at step 10000\nEpisode 22 Total reward: 2215.0, Max x: 1513\nEpisode 23 Total reward: 1199.0, Max x: 843\nEpisode 24 Total reward: 2376.0, Max x: 1135\nEpisode 25 Total reward: 2213.0, Max x: 1130\nEpisode 26 Total reward: 1830.0, Max x: 1138\nEpisode 27 Total reward: 2763.0, Max x: 1138\nEpisode 28 Total reward: 1187.0, Max x: 826\nEpisode 29 Total reward: 1939.0, Max x: 819\nEpisode 30 Total reward: 1885.0, Max x: 1138\nEpisode 31 Total reward: 1874.0, Max x: 1130\nEpisode 32 Total reward: 1672.0, Max x: 824\n0.3886637381784849\nEpisode 33 Total reward: 1823.0, Max x: 1136\nEpisode 34 Total reward: 1878.0, Max x: 1138\nEpisode 35 Total reward: 2255.0, Max x: 1133\nEpisode 36 Total reward: 1532.0, Max x: 1501\nEpisode 37 Total reward: 2059.0, Max x: 826\nEpisode 38 Total reward: 3359.0, Max x: 1414\nEpisode 39 Total reward: 2168.0, Max x: 1411\nEpisode 40 Total reward: 2674.0, Max x: 1138\nEpisode 41 Total reward: 2560.0, Max x: 1415\nEpisode 42 Total reward: 1052.0, Max x: 687\nEpisode 43 Total reward: 659.0, Max x: 296\nEpisode 44 Total reward: 1075.0, Max x: 704\nEpisode 45 Total reward: 1989.0, Max x: 1135\n0.3881782119621075\nMarioNet saved to checkpoints/2024-05-23T08-34-53/mario_net_2.chkpt at step 20000\nEpisode 46 Total reward: 1591.0, Max x: 1410\nEpisode 47 Total reward: 1435.0, Max x: 702\nEpisode 48 Total reward: 1418.0, Max x: 678\nEpisode 49 Total reward: 662.0, Max x: 296\nEpisode 50 Total reward: 2476.0, Max x: 1952\nEpisode 51 Total reward: 1752.0, Max x: 1415\nEpisode 52 Total reward: 2022.0, Max x: 1136\nEpisode 53 Total reward: 1953.0, Max x: 1126\nEpisode 54 Total reward: 1445.0, Max x: 1425\nEpisode 55 Total reward: 3628.0, Max x: 1418\nEpisode 56 Total reward: 2538.0, Max x: 1404\nEpisode 57 Total reward: 2560.0, Max x: 1413\nEpisode 58 Total reward: 1874.0, Max x: 1127\nEpisode 59 Total reward: 1071.0, Max x: 702\n0.38769329227441535\nEpisode 60 Total reward: 2430.0, Max x: 1138\nEpisode 61 Total reward: 2656.0, Max x: 1138\nEpisode 62 Total reward: 1998.0, Max x: 1414\nEpisode 63 Total reward: 2265.0, Max x: 1403\nEpisode 64 Total reward: 1944.0, Max x: 819\nEpisode 65 Total reward: 1075.0, Max x: 702\nEpisode 66 Total reward: 1875.0, Max x: 1125\nEpisode 67 Total reward: 2324.0, Max x: 1228\nEpisode 68 Total reward: 1889.0, Max x: 1138\nEpisode 69 Total reward: 1806.0, Max x: 694\nEpisode 70 Total reward: 1937.0, Max x: 1228\nEpisode 71 Total reward: 1451.0, Max x: 698\nEpisode 72 Total reward: 1075.0, Max x: 704\n0.38720897835772633\nMarioNet saved to checkpoints/2024-05-23T08-34-53/mario_net_3.chkpt at step 30000\nEpisode 73 Total reward: 1452.0, Max x: 701\nEpisode 74 Total reward: 1964.0, Max x: 1135\nEpisode 75 Total reward: 1075.0, Max x: 704\nEpisode 76 Total reward: 2644.0, Max x: 1127\nEpisode 77 Total reward: 1590.0, Max x: 1416\nEpisode 78 Total reward: 2130.0, Max x: 1410\nEpisode 79 Total reward: 1829.0, Max x: 1125\nEpisode 80 Total reward: 1853.0, Max x: 1135\nEpisode 81 Total reward: 2169.0, Max x: 898\nEpisode 82 Total reward: 2418.0, Max x: 1414\nEpisode 83 Total reward: 1074.0, Max x: 702\nEpisode 84 Total reward: 1644.0, Max x: 898\nEpisode 85 Total reward: 1797.0, Max x: 691\n0.38672526945529506\nEpisode 86 Total reward: 1814.0, Max x: 702\nEpisode 87 Total reward: 1460.0, Max x: 702\nEpisode 88 Total reward: 1947.0, Max x: 798\nEpisode 89 Total reward: 1947.0, Max x: 1404\nEpisode 90 Total reward: 1984.0, Max x: 868\nEpisode 91 Total reward: 1832.0, Max x: 899\nEpisode 92 Total reward: 1476.0, Max x: 1128\nEpisode 93 Total reward: 1430.0, Max x: 691\nEpisode 94 Total reward: 1439.0, Max x: 1414\nEpisode 95 Total reward: 1423.0, Max x: 1406\nEpisode 96 Total reward: 2274.0, Max x: 1136\nEpisode 97 Total reward: 3515.0, Max x: 2470\nEpisode 98 Total reward: 1457.0, Max x: 1126\nEpisode 99 Total reward: 2556.0, Max x: 1415\n0.3862421648113291\nMarioNet saved to checkpoints/2024-05-23T08-34-53/mario_net_4.chkpt at step 40000\nEpisode 100 Total reward: 1453.0, Max x: 723\nEpisode 101 Total reward: 1520.0, Max x: 1498\nEpisode 102 Total reward: 674.0, Max x: 296\nEpisode 103 Total reward: 1604.0, Max x: 852\nEpisode 104 Total reward: 1587.0, Max x: 1410\nEpisode 105 Total reward: 2369.0, Max x: 1136\nEpisode 106 Total reward: 2926.0, Max x: 1409\nEpisode 107 Total reward: 2244.0, Max x: 1136\nEpisode 108 Total reward: 1818.0, Max x: 702\nEpisode 109 Total reward: 1840.0, Max x: 1126\nEpisode 110 Total reward: 1529.0, Max x: 1502\nEpisode 111 Total reward: 1524.0, Max x: 1493\nEpisode 112 Total reward: 1870.0, Max x: 1138\nEpisode 113 Total reward: 1482.0, Max x: 1135\n0.38575966367097386\nEpisode 114 Total reward: 1845.0, Max x: 1137\nEpisode 115 Total reward: 1851.0, Max x: 1137\nEpisode 116 Total reward: 2291.0, Max x: 1138\nEpisode 117 Total reward: 1438.0, Max x: 691\nEpisode 118 Total reward: 1055.0, Max x: 682\nEpisode 119 Total reward: 1420.0, Max x: 699\nEpisode 120 Total reward: 1577.0, Max x: 827\nEpisode 121 Total reward: 2497.0, Max x: 1401\nEpisode 122 Total reward: 2934.0, Max x: 1401\nEpisode 123 Total reward: 2286.0, Max x: 1133\nEpisode 124 Total reward: 2794.0, Max x: 1250\nEpisode 125 Total reward: 1412.0, Max x: 686\n0.38527776528032226\nMarioNet saved to checkpoints/2024-05-23T08-34-53/mario_net_5.chkpt at step 50000\nEpisode 126 Total reward: 2302.0, Max x: 1139\nEpisode 127 Total reward: 2111.0, Max x: 1421\nEpisode 128 Total reward: 1481.0, Max x: 1126\nEpisode 129 Total reward: 1496.0, Max x: 1135\nEpisode 130 Total reward: 1054.0, Max x: 680\nEpisode 131 Total reward: 1489.0, Max x: 1137\nEpisode 132 Total reward: 1481.0, Max x: 1125\nEpisode 133 Total reward: 1476.0, Max x: 1124\nEpisode 134 Total reward: 2374.0, Max x: 1138\nEpisode 135 Total reward: 1763.0, Max x: 1414\nEpisode 136 Total reward: 1075.0, Max x: 701\nEpisode 137 Total reward: 1470.0, Max x: 1125\nEpisode 138 Total reward: 1956.0, Max x: 1222\nEpisode 139 Total reward: 1845.0, Max x: 1137\nEpisode 140 Total reward: 2105.0, Max x: 1401\nEpisode 141 Total reward: 1393.0, Max x: 668\n0.38479646888640795\nEpisode 142 Total reward: 1483.0, Max x: 1137\nEpisode 143 Total reward: 1986.0, Max x: 1410\nEpisode 144 Total reward: 3608.0, Max x: 1414\nEpisode 145 Total reward: 1434.0, Max x: 1414\nEpisode 146 Total reward: 1688.0, Max x: 1509\nEpisode 147 Total reward: 2256.0, Max x: 1134\nEpisode 148 Total reward: 1948.0, Max x: 803\nEpisode 149 Total reward: 1237.0, Max x: 899\nEpisode 150 Total reward: 1861.0, Max x: 1137\nEpisode 151 Total reward: 680.0, Max x: 296\nEpisode 152 Total reward: 1536.0, Max x: 810\nEpisode 153 Total reward: 2241.0, Max x: 1126\n0.3843157737372056\nMarioNet saved to checkpoints/2024-05-23T08-34-53/mario_net_6.chkpt at step 60000\nEpisode 154 Total reward: 1872.0, Max x: 1138\nEpisode 155 Total reward: 1668.0, Max x: 1503\nEpisode 156 Total reward: 2648.0, Max x: 1410\nEpisode 157 Total reward: 3068.0, Max x: 1408\nEpisode 158 Total reward: 1564.0, Max x: 819\nEpisode 159 Total reward: 1488.0, Max x: 1137\nEpisode 160 Total reward: 1941.0, Max x: 1130\nEpisode 161 Total reward: 2242.0, Max x: 1125\nEpisode 162 Total reward: 1076.0, Max x: 704\nEpisode 163 Total reward: 1881.0, Max x: 1138\nEpisode 164 Total reward: 2019.0, Max x: 898\nEpisode 165 Total reward: 2207.0, Max x: 1130\n0.38383567908162636\nEpisode 166 Total reward: 1476.0, Max x: 722\nEpisode 167 Total reward: 1830.0, Max x: 723\nEpisode 168 Total reward: 2261.0, Max x: 1135\nEpisode 169 Total reward: 1865.0, Max x: 1134\nEpisode 170 Total reward: 1770.0, Max x: 1425\nEpisode 171 Total reward: 1997.0, Max x: 898\nEpisode 172 Total reward: 1950.0, Max x: 1786\nEpisode 173 Total reward: 2304.0, Max x: 1138\nEpisode 174 Total reward: 2922.0, Max x: 1513\nEpisode 175 Total reward: 680.0, Max x: 296\nEpisode 176 Total reward: 3466.0, Max x: 2753\nEpisode 177 Total reward: 2257.0, Max x: 1139\n0.3833561841695256\nMarioNet saved to checkpoints/2024-05-23T08-34-53/mario_net_7.chkpt at step 70000\nEpisode 178 Total reward: 2651.0, Max x: 1137\nEpisode 179 Total reward: 2625.0, Max x: 1411\nEpisode 180 Total reward: 2808.0, Max x: 1410\nEpisode 181 Total reward: 1417.0, Max x: 699\nEpisode 182 Total reward: 1581.0, Max x: 1404\nEpisode 183 Total reward: 2392.0, Max x: 1126\nEpisode 184 Total reward: 1476.0, Max x: 723\nEpisode 185 Total reward: 1843.0, Max x: 1131\nEpisode 186 Total reward: 2944.0, Max x: 1403\nEpisode 187 Total reward: 1889.0, Max x: 1139\n0.3828772882516893\nEpisode 188 Total reward: 4725.0, Max x: 3157\nEpisode 189 Total reward: 1809.0, Max x: 691\nEpisode 190 Total reward: 1204.0, Max x: 843\nEpisode 191 Total reward: 680.0, Max x: 296\nEpisode 192 Total reward: 1513.0, Max x: 1499\nEpisode 193 Total reward: 1960.0, Max x: 794\nEpisode 194 Total reward: 3155.0, Max x: 2471\nEpisode 195 Total reward: 1575.0, Max x: 854\nEpisode 196 Total reward: 1530.0, Max x: 1509\nEpisode 197 Total reward: 2119.0, Max x: 2004\nEpisode 198 Total reward: 680.0, Max x: 296\nEpisode 199 Total reward: 1892.0, Max x: 1140\nEpisode 200 Total reward: 1578.0, Max x: 1408\nEpisode 201 Total reward: 2421.0, Max x: 1138\nEpisode 202 Total reward: 2243.0, Max x: 1136\nEpisode 203 Total reward: 680.0, Max x: 296\n0.3823989905798453\nMarioNet saved to checkpoints/2024-05-23T08-34-53/mario_net_8.chkpt at step 80000\nEpisode 204 Total reward: 1429.0, Max x: 1410\nEpisode 205 Total reward: 1953.0, Max x: 1414\nEpisode 206 Total reward: 2063.0, Max x: 1138\nEpisode 207 Total reward: 2243.0, Max x: 1411\nEpisode 208 Total reward: 1932.0, Max x: 798\nEpisode 209 Total reward: 2527.0, Max x: 1410\nEpisode 210 Total reward: 2390.0, Max x: 1415\nEpisode 211 Total reward: 2055.0, Max x: 1135\nEpisode 212 Total reward: 1074.0, Max x: 702\nEpisode 213 Total reward: 1988.0, Max x: 1125\nEpisode 214 Total reward: 2440.0, Max x: 1137\nEpisode 215 Total reward: 2228.0, Max x: 1648\n0.3819212904066498\nEpisode 216 Total reward: 680.0, Max x: 296\nEpisode 217 Total reward: 3370.0, Max x: 1427\nEpisode 218 Total reward: 2647.0, Max x: 1138\nEpisode 219 Total reward: 1985.0, Max x: 1138\nEpisode 220 Total reward: 1469.0, Max x: 704\nEpisode 221 Total reward: 1209.0, Max x: 847\nEpisode 222 Total reward: 1561.0, Max x: 838\nEpisode 223 Total reward: 2651.0, Max x: 1507\nEpisode 224 Total reward: 1690.0, Max x: 1508\nEpisode 225 Total reward: 1872.0, Max x: 1138\nEpisode 226 Total reward: 1082.0, Max x: 722\nEpisode 227 Total reward: 1878.0, Max x: 1137\nEpisode 228 Total reward: 2067.0, Max x: 1133\n0.38144418698569893\nMarioNet saved to checkpoints/2024-05-23T08-34-53/mario_net_9.chkpt at step 90000\nEpisode 229 Total reward: 2060.0, Max x: 1509\nEpisode 230 Total reward: 1879.0, Max x: 1138\nEpisode 231 Total reward: 1822.0, Max x: 701\nEpisode 232 Total reward: 1755.0, Max x: 1412\nEpisode 233 Total reward: 680.0, Max x: 296\nEpisode 234 Total reward: 1886.0, Max x: 1137\nEpisode 235 Total reward: 1878.0, Max x: 1137\nEpisode 236 Total reward: 2408.0, Max x: 1126\nEpisode 237 Total reward: 1464.0, Max x: 702\nEpisode 238 Total reward: 1522.0, Max x: 807\nEpisode 239 Total reward: 1774.0, Max x: 898\nEpisode 240 Total reward: 1860.0, Max x: 1131\nEpisode 241 Total reward: 1635.0, Max x: 898\nEpisode 242 Total reward: 2397.0, Max x: 1138\n0.380967679571517\nEpisode 243 Total reward: 1583.0, Max x: 820\nEpisode 244 Total reward: 2285.0, Max x: 1124\nEpisode 245 Total reward: 1432.0, Max x: 1413\nEpisode 246 Total reward: 1872.0, Max x: 1126\nEpisode 247 Total reward: 1450.0, Max x: 702\nEpisode 248 Total reward: 2389.0, Max x: 1130\nEpisode 249 Total reward: 2689.0, Max x: 1410\nEpisode 250 Total reward: 2234.0, Max x: 1127\nEpisode 251 Total reward: 1074.0, Max x: 704\nEpisode 252 Total reward: 1431.0, Max x: 1423\nEpisode 253 Total reward: 2678.0, Max x: 1138\nEpisode 254 Total reward: 1447.0, Max x: 704\nEpisode 255 Total reward: 1886.0, Max x: 1138\nEpisode 256 Total reward: 1451.0, Max x: 696\n0.3804917674195591\nMarioNet saved to checkpoints/2024-05-23T08-34-53/mario_net_10.chkpt at step 100000\nEpisode 257 Total reward: 1606.0, Max x: 1416\nEpisode 258 Total reward: 1427.0, Max x: 694\nEpisode 259 Total reward: 2450.0, Max x: 2469\nEpisode 260 Total reward: 2168.0, Max x: 898\nEpisode 261 Total reward: 1043.0, Max x: 675\nEpisode 262 Total reward: 1878.0, Max x: 1133\nEpisode 263 Total reward: 1212.0, Max x: 854\nEpisode 264 Total reward: 1482.0, Max x: 1125\nEpisode 265 Total reward: 2447.0, Max x: 1228\nEpisode 266 Total reward: 1418.0, Max x: 1410\nEpisode 267 Total reward: 2293.0, Max x: 1138\nEpisode 268 Total reward: 1568.0, Max x: 1227\nEpisode 269 Total reward: 1435.0, Max x: 1413\nEpisode 270 Total reward: 1850.0, Max x: 1125\n0.38001644978621596\nEpisode 271 Total reward: 1425.0, Max x: 1414\nEpisode 272 Total reward: 2410.0, Max x: 1138\nEpisode 273 Total reward: 2362.0, Max x: 1138\nEpisode 274 Total reward: 1601.0, Max x: 850\nEpisode 275 Total reward: 1875.0, Max x: 1125\nEpisode 276 Total reward: 2272.0, Max x: 1136\nEpisode 277 Total reward: 1400.0, Max x: 673\nEpisode 278 Total reward: 2916.0, Max x: 1402\nEpisode 279 Total reward: 1436.0, Max x: 1425\nEpisode 280 Total reward: 2251.0, Max x: 1138\nEpisode 281 Total reward: 672.0, Max x: 296\nEpisode 282 Total reward: 1961.0, Max x: 1412\nEpisode 283 Total reward: 680.0, Max x: 296\nEpisode 284 Total reward: 1847.0, Max x: 704\n0.379541725928798\nMarioNet saved to checkpoints/2024-05-23T08-34-53/mario_net_11.chkpt at step 110000\nEpisode 285 Total reward: 1822.0, Max x: 1135\nEpisode 286 Total reward: 1859.0, Max x: 1138\nEpisode 287 Total reward: 1874.0, Max x: 1129\nEpisode 288 Total reward: 2499.0, Max x: 1502\nEpisode 289 Total reward: 1853.0, Max x: 706\nEpisode 290 Total reward: 2011.0, Max x: 1138\nEpisode 291 Total reward: 1692.0, Max x: 1502\nEpisode 292 Total reward: 1159.0, Max x: 789\nEpisode 293 Total reward: 2232.0, Max x: 1129\nEpisode 294 Total reward: 1675.0, Max x: 1662\nEpisode 295 Total reward: 1462.0, Max x: 702\nEpisode 296 Total reward: 1426.0, Max x: 1421\nEpisode 297 Total reward: 1533.0, Max x: 1515\n0.37906759510555604\nEpisode 298 Total reward: 2241.0, Max x: 1135\nEpisode 299 Total reward: 1615.0, Max x: 899\nEpisode 300 Total reward: 2225.0, Max x: 1127\nEpisode 301 Total reward: 2561.0, Max x: 1133\nEpisode 302 Total reward: 2780.0, Max x: 1135\nEpisode 303 Total reward: 1475.0, Max x: 1124\nEpisode 304 Total reward: 1458.0, Max x: 703\nEpisode 305 Total reward: 1435.0, Max x: 691\nEpisode 306 Total reward: 1447.0, Max x: 723\nEpisode 307 Total reward: 1995.0, Max x: 1416\nEpisode 308 Total reward: 2110.0, Max x: 1227\nEpisode 309 Total reward: 2304.0, Max x: 1131\nEpisode 310 Total reward: 1557.0, Max x: 847\n0.37859405657565526\nMarioNet saved to checkpoints/2024-05-23T08-34-53/mario_net_12.chkpt at step 120000\nEpisode 311 Total reward: 1797.0, Max x: 698\nEpisode 312 Total reward: 1041.0, Max x: 673\nEpisode 313 Total reward: 2351.0, Max x: 1126\nEpisode 314 Total reward: 2406.0, Max x: 1131\nEpisode 315 Total reward: 1489.0, Max x: 1131\nEpisode 316 Total reward: 2272.0, Max x: 1135\nEpisode 317 Total reward: 1456.0, Max x: 698\nEpisode 318 Total reward: 1939.0, Max x: 798\nEpisode 319 Total reward: 1058.0, Max x: 687\nEpisode 320 Total reward: 1163.0, Max x: 808\nEpisode 321 Total reward: 1941.0, Max x: 808\nEpisode 322 Total reward: 2930.0, Max x: 1410\nEpisode 323 Total reward: 2516.0, Max x: 1137\n0.3781211095991942\nEpisode 324 Total reward: 1888.0, Max x: 1782\nEpisode 325 Total reward: 1394.0, Max x: 668\nEpisode 326 Total reward: 1421.0, Max x: 687\nEpisode 327 Total reward: 2552.0, Max x: 1402\nEpisode 328 Total reward: 1431.0, Max x: 703\nEpisode 329 Total reward: 3021.0, Max x: 1415\nEpisode 330 Total reward: 2053.0, Max x: 899\nEpisode 331 Total reward: 1982.0, Max x: 1419\nEpisode 332 Total reward: 1418.0, Max x: 1414\nEpisode 333 Total reward: 1744.0, Max x: 1422\nEpisode 334 Total reward: 2513.0, Max x: 1124\nEpisode 335 Total reward: 2014.0, Max x: 1138\n0.3776487534371909\nMarioNet saved to checkpoints/2024-05-23T08-34-53/mario_net_13.chkpt at step 130000\nEpisode 336 Total reward: 1935.0, Max x: 827\nEpisode 337 Total reward: 1941.0, Max x: 1222\nEpisode 338 Total reward: 675.0, Max x: 296\nEpisode 339 Total reward: 1054.0, Max x: 691\nEpisode 340 Total reward: 2844.0, Max x: 1138\nEpisode 341 Total reward: 2704.0, Max x: 1509\nEpisode 342 Total reward: 1425.0, Max x: 702\nEpisode 343 Total reward: 1887.0, Max x: 1138\nEpisode 344 Total reward: 2008.0, Max x: 1138\nEpisode 345 Total reward: 1035.0, Max x: 682\nEpisode 346 Total reward: 2034.0, Max x: 1138\nEpisode 347 Total reward: 2831.0, Max x: 1138\n0.37717698735158955\nEpisode 348 Total reward: 1872.0, Max x: 1138\nEpisode 349 Total reward: 2010.0, Max x: 1138\nEpisode 350 Total reward: 1408.0, Max x: 1410\nEpisode 351 Total reward: 1827.0, Max x: 1126\nEpisode 352 Total reward: 1987.0, Max x: 1410\nEpisode 353 Total reward: 1478.0, Max x: 1130\nEpisode 354 Total reward: 1437.0, Max x: 723\nEpisode 355 Total reward: 2249.0, Max x: 1138\nEpisode 356 Total reward: 1524.0, Max x: 1511\nEpisode 357 Total reward: 2028.0, Max x: 2009\nEpisode 358 Total reward: 2192.0, Max x: 1509\nEpisode 359 Total reward: 2144.0, Max x: 1410\n0.37670581060525576\nMarioNet saved to checkpoints/2024-05-23T08-34-53/mario_net_14.chkpt at step 140000\nEpisode 360 Total reward: 1499.0, Max x: 798\nEpisode 361 Total reward: 2648.0, Max x: 1942\nEpisode 362 Total reward: 1508.0, Max x: 1500\nEpisode 363 Total reward: 1435.0, Max x: 701\nEpisode 364 Total reward: 1406.0, Max x: 686\nEpisode 365 Total reward: 2055.0, Max x: 1134\nEpisode 366 Total reward: 1798.0, Max x: 1781\nEpisode 367 Total reward: 1990.0, Max x: 843\nEpisode 368 Total reward: 2651.0, Max x: 1138\nEpisode 369 Total reward: 1421.0, Max x: 682\nEpisode 370 Total reward: 1216.0, Max x: 855\nEpisode 371 Total reward: 1151.0, Max x: 791\nEpisode 372 Total reward: 1469.0, Max x: 704\nEpisode 373 Total reward: 1048.0, Max x: 696\nEpisode 374 Total reward: 1059.0, Max x: 691\n0.3762352224619743\nEpisode 375 Total reward: 1923.0, Max x: 1124\nEpisode 376 Total reward: 1458.0, Max x: 704\nEpisode 377 Total reward: 1165.0, Max x: 814\nEpisode 378 Total reward: 1884.0, Max x: 1126\nEpisode 379 Total reward: 2950.0, Max x: 1411\nEpisode 380 Total reward: 1601.0, Max x: 850\nEpisode 381 Total reward: 2122.0, Max x: 1406\nEpisode 382 Total reward: 1474.0, Max x: 1129\nEpisode 383 Total reward: 2650.0, Max x: 1137\nEpisode 384 Total reward: 3111.0, Max x: 1412\nEpisode 385 Total reward: 1068.0, Max x: 702\n0.3757652221864497\nMarioNet saved to checkpoints/2024-05-23T08-34-53/mario_net_15.chkpt at step 150000\nEpisode 386 Total reward: 2787.0, Max x: 1138\nEpisode 387 Total reward: 1068.0, Max x: 696\nEpisode 388 Total reward: 1155.0, Max x: 799\nEpisode 389 Total reward: 1971.0, Max x: 1138\nEpisode 390 Total reward: 1609.0, Max x: 866\nEpisode 391 Total reward: 2346.0, Max x: 1138\nEpisode 392 Total reward: 2273.0, Max x: 1138\nEpisode 393 Total reward: 1597.0, Max x: 1425\nEpisode 394 Total reward: 1957.0, Max x: 814\nEpisode 395 Total reward: 1424.0, Max x: 704\nEpisode 396 Total reward: 3377.0, Max x: 1412\nEpisode 397 Total reward: 1451.0, Max x: 1135\nEpisode 398 Total reward: 2700.0, Max x: 1136\n0.3752958090443103\nEpisode 399 Total reward: 680.0, Max x: 296\nEpisode 400 Total reward: 1723.0, Max x: 854\nEpisode 401 Total reward: 1805.0, Max x: 689\nEpisode 402 Total reward: 1428.0, Max x: 1414\nEpisode 403 Total reward: 1873.0, Max x: 1138\nEpisode 404 Total reward: 1436.0, Max x: 1424\nEpisode 405 Total reward: 2260.0, Max x: 1137\nEpisode 406 Total reward: 1582.0, Max x: 850\nEpisode 407 Total reward: 1833.0, Max x: 722\nEpisode 408 Total reward: 1594.0, Max x: 1414\nEpisode 409 Total reward: 1434.0, Max x: 1418\nEpisode 410 Total reward: 1521.0, Max x: 1520\nEpisode 411 Total reward: 1574.0, Max x: 826\nEpisode 412 Total reward: 2106.0, Max x: 1780\n0.37482698230209577\nMarioNet saved to checkpoints/2024-05-23T08-34-53/mario_net_16.chkpt at step 160000\nEpisode 413 Total reward: 1189.0, Max x: 824\nEpisode 414 Total reward: 3040.0, Max x: 1409\nEpisode 415 Total reward: 1988.0, Max x: 1138\nEpisode 416 Total reward: 1433.0, Max x: 1414\nEpisode 417 Total reward: 3067.0, Max x: 1422\nEpisode 418 Total reward: 2218.0, Max x: 1135\nEpisode 419 Total reward: 2011.0, Max x: 1138\nEpisode 420 Total reward: 1534.0, Max x: 810\nEpisode 421 Total reward: 2590.0, Max x: 1137\nEpisode 422 Total reward: 1878.0, Max x: 1137\n0.37435874122726065\nEpisode 423 Total reward: 1427.0, Max x: 692\nEpisode 424 Total reward: 3191.0, Max x: 1411\nEpisode 425 Total reward: 2678.0, Max x: 1139\nEpisode 426 Total reward: 2020.0, Max x: 1135\nEpisode 427 Total reward: 1841.0, Max x: 1134\nEpisode 428 Total reward: 1479.0, Max x: 722\nEpisode 429 Total reward: 2213.0, Max x: 1135\nEpisode 430 Total reward: 1411.0, Max x: 687\nEpisode 431 Total reward: 2649.0, Max x: 1418\nEpisode 432 Total reward: 1177.0, Max x: 813\nEpisode 433 Total reward: 1431.0, Max x: 1414\nEpisode 434 Total reward: 1146.0, Max x: 798\nEpisode 435 Total reward: 1063.0, Max x: 691\n0.3738910850881812\nMarioNet saved to checkpoints/2024-05-23T08-34-53/mario_net_17.chkpt at step 170000\nEpisode 436 Total reward: 1863.0, Max x: 1136\nEpisode 437 Total reward: 2014.0, Max x: 1427\nEpisode 438 Total reward: 1445.0, Max x: 1414\nEpisode 439 Total reward: 1082.0, Max x: 722\nEpisode 440 Total reward: 1876.0, Max x: 1135\nEpisode 441 Total reward: 1443.0, Max x: 1422\nEpisode 442 Total reward: 2668.0, Max x: 1138\nEpisode 443 Total reward: 2497.0, Max x: 1401\nEpisode 444 Total reward: 3048.0, Max x: 1414\nEpisode 445 Total reward: 2788.0, Max x: 1410\n0.37342401315414625\nEpisode 446 Total reward: 2055.0, Max x: 1138\nEpisode 447 Total reward: 1950.0, Max x: 1941\nEpisode 448 Total reward: 2566.0, Max x: 1410\nEpisode 449 Total reward: 2259.0, Max x: 1138\nEpisode 450 Total reward: 1482.0, Max x: 1135\nEpisode 451 Total reward: 1882.0, Max x: 1135\nEpisode 452 Total reward: 1427.0, Max x: 704\nEpisode 453 Total reward: 2266.0, Max x: 1138\nEpisode 454 Total reward: 1827.0, Max x: 1651\nEpisode 455 Total reward: 1603.0, Max x: 1421\nEpisode 456 Total reward: 1718.0, Max x: 1783\nEpisode 457 Total reward: 2284.0, Max x: 1138\n0.37295752469535587\nMarioNet saved to checkpoints/2024-05-23T08-34-53/mario_net_18.chkpt at step 180000\nEpisode 458 Total reward: 1957.0, Max x: 1133\nEpisode 459 Total reward: 1468.0, Max x: 702\nEpisode 460 Total reward: 2116.0, Max x: 1425\nEpisode 461 Total reward: 1392.0, Max x: 1413\nEpisode 462 Total reward: 1878.0, Max x: 1137\nEpisode 463 Total reward: 2004.0, Max x: 856\nEpisode 464 Total reward: 2748.0, Max x: 1407\nEpisode 465 Total reward: 2161.0, Max x: 836\nEpisode 466 Total reward: 2254.0, Max x: 1130\nEpisode 467 Total reward: 2292.0, Max x: 1138\nEpisode 468 Total reward: 1448.0, Max x: 1424\n0.3724916189829189\nEpisode 469 Total reward: 2533.0, Max x: 1404\nEpisode 470 Total reward: 1566.0, Max x: 833\nEpisode 471 Total reward: 1438.0, Max x: 702\nEpisode 472 Total reward: 1662.0, Max x: 898\n","output_type":"stream"}]},{"cell_type":"code","source":"!zip -r file.zip /kaggle/working/checkpoints/2024-05-22T18-22-07/","metadata":{"execution":{"iopub.status.busy":"2024-05-23T05:02:05.666015Z","iopub.execute_input":"2024-05-23T05:02:05.666304Z","iopub.status.idle":"2024-05-23T05:02:06.658458Z","shell.execute_reply.started":"2024-05-23T05:02:05.666278Z","shell.execute_reply":"2024-05-23T05:02:06.656974Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\tzip warning: name not matched: /kaggle/working/checkpoints/2024-05-22T18-22-07/\n\nzip error: Nothing to do! (try: zip -r file.zip . -i /kaggle/working/checkpoints/2024-05-22T18-22-07/)\n","output_type":"stream"}]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'file.zip')","metadata":{"execution":{"iopub.status.busy":"2024-05-22T13:56:38.714712Z","iopub.execute_input":"2024-05-22T13:56:38.715898Z","iopub.status.idle":"2024-05-22T13:56:38.725215Z","shell.execute_reply.started":"2024-05-22T13:56:38.715853Z","shell.execute_reply":"2024-05-22T13:56:38.724040Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/file.zip","text/html":"<a href='file.zip' target='_blank'>file.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"!rm -r /kaggle/working/checkpoints/2024-05-22T09-01-45/","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:00:03.648319Z","iopub.execute_input":"2024-05-22T10:00:03.649035Z","iopub.status.idle":"2024-05-22T10:00:04.602702Z","shell.execute_reply.started":"2024-05-22T10:00:03.648999Z","shell.execute_reply":"2024-05-22T10:00:04.601433Z"},"trusted":true},"execution_count":9,"outputs":[]}]}